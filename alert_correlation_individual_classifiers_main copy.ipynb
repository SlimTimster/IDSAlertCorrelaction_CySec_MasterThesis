{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Custom Imports\n",
    "from helpers.data_helper import load_data_robust, load_models_from_disk, classify_unseen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/russellmitchell\"\n",
    "\n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/harrison\"\n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/shaw\"    \n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/wheeler\" \n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/wardbeck\"\n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/fox\"     \n",
    "#PATH_DATASET_TO_CLASSIFY = \"../AIT_LD-v2/wilson\"  \n",
    "\n",
    "# -> santos was used to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get events from file: intranet / auth.log\n",
    "* Contributes:\n",
    "  * Compromised User Account -> IOC and Asset\n",
    "  * Root Access Events -> Event\n",
    "  * Which files were accessed -> Event\n",
    "  * Which commands were executed and in which PWD -> Event\n",
    "* Trained Classifiers: \n",
    "  * RandomForest \n",
    "  * GradientBoost \n",
    "  * SVM\n",
    "  * MLP \n",
    "* Uses binary classification\n",
    "* Features are simply booleans depending on presence of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by Step:\n",
    "\n",
    "# 1. Import log file\n",
    "# 2. Extract features from log file to be used for classification\n",
    "# 3. Load trained classifier(s)\n",
    "# 4. Classify log file\n",
    "# 5. Keep attack-related logs\n",
    "\n",
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import log file\n",
    "\n",
    "path_intranet_auth_log = \"/gather/intranet_server/logs/auth.log\"\n",
    "df_intranet_auth = load_data_robust(PATH_DATASET_TO_CLASSIFY + path_intranet_auth_log)\n",
    "\n",
    "#df_intranet_auth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract features from log file to be used for classification\n",
    "\n",
    "from helpers.intranet_auth_log_helper import extract_features\n",
    "\n",
    "df_intranet_auth_features = extract_features(df_intranet_auth)\n",
    "\n",
    "#df_intranet_auth_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded intr_auth_gradientboost from trained-models/intranet_auth_log\\intr_auth_gradientboost.joblib\n",
      "Loaded intr_auth_mlp from trained-models/intranet_auth_log\\intr_auth_mlp.joblib\n",
      "Loaded intr_auth_randomforest from trained-models/intranet_auth_log\\intr_auth_randomforest.joblib\n",
      "Loaded intr_auth_svc from trained-models/intranet_auth_log\\intr_auth_svc.joblib\n"
     ]
    }
   ],
   "source": [
    "# 3. Load trained classifier(s)\n",
    "\n",
    "models = load_models_from_disk(\"trained-models/intranet_auth_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intr_auth_gradientboost': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'intr_auth_mlp': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'intr_auth_randomforest': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'intr_auth_svc': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Classify log file\n",
    "all_predictions = classify_unseen_data(models, df_intranet_auth_features)\n",
    "\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get some Stats / Evaluations, choose predictions of best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose predictions of one model\n",
    "\n",
    "predictions_grad_boost = all_predictions[\"intr_auth_gradientboost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Jan 23 16:23:04</td>\n",
       "      <td>systemd-logind[957]: Removed session 111.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Jan 23 16:30:47</td>\n",
       "      <td>systemd: pam_unix(systemd-user:session): sessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Jan 23 16:30:47</td>\n",
       "      <td>systemd-logind[957]: New session 271 of user j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Jan 24 04:37:40</td>\n",
       "      <td>su[27950]: Successful su for jhall by www-data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Jan 24 04:37:40</td>\n",
       "      <td>su[27950]: + /dev/pts/1 www-data:jhall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Jan 24 04:37:40</td>\n",
       "      <td>su[27950]: pam_unix(su:session): session opene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Jan 24 04:37:40</td>\n",
       "      <td>systemd-logind[957]: New session c1 of user jh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Jan 24 04:37:58</td>\n",
       "      <td>sudo:    jhall : TTY=pts/1 ; PWD=/var/www/intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Jan 24 04:38:06</td>\n",
       "      <td>sudo:    jhall : TTY=pts/1 ; PWD=/var/www/intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Jan 24 04:38:06</td>\n",
       "      <td>sudo: pam_unix(sudo:session): session opened f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Jan 24 04:38:06</td>\n",
       "      <td>sudo: pam_unix(sudo:session): session closed f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp                                            message\n",
       "66   Jan 23 16:23:04          systemd-logind[957]: Removed session 111.\n",
       "69   Jan 23 16:30:47  systemd: pam_unix(systemd-user:session): sessi...\n",
       "70   Jan 23 16:30:47  systemd-logind[957]: New session 271 of user j...\n",
       "144  Jan 24 04:37:40     su[27950]: Successful su for jhall by www-data\n",
       "145  Jan 24 04:37:40             su[27950]: + /dev/pts/1 www-data:jhall\n",
       "146  Jan 24 04:37:40  su[27950]: pam_unix(su:session): session opene...\n",
       "147  Jan 24 04:37:40  systemd-logind[957]: New session c1 of user jh...\n",
       "148  Jan 24 04:37:58  sudo:    jhall : TTY=pts/1 ; PWD=/var/www/intr...\n",
       "149  Jan 24 04:38:06  sudo:    jhall : TTY=pts/1 ; PWD=/var/www/intr...\n",
       "150  Jan 24 04:38:06  sudo: pam_unix(sudo:session): session opened f...\n",
       "151  Jan 24 04:38:06  sudo: pam_unix(sudo:session): session closed f..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Keep attack-related logs\n",
    "\n",
    "df_attack_related_intranet_auth = df_intranet_auth[predictions_grad_boost == 1]\n",
    "\n",
    "df_attack_related_intranet_auth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get suspicious events and info from the dnsmasq logs\n",
    "* Contributes:\n",
    "  * IP address of where files are extracted to\n",
    "  * potentially file names of extracted files\n",
    "  * Time(frame) of extraction\n",
    "* Trained Classifiers: \n",
    "  * RandomForest for MultiClass classification\n",
    "\n",
    "* Includes DNS Exfiltration Attack Step\n",
    "* Extract domain and message type using regex pattern matching\n",
    "* Message types:\n",
    "  * queries (a, aaaa, srv, txt, ptr, ,mx)\n",
    "  * forwarded\n",
    "  * reply\n",
    "  * cached\n",
    "  * nameserver\n",
    "* Domain features:\n",
    "  * Lenght\n",
    "  * Parts\n",
    "  * Avg_part_length\n",
    "  * Max_part_length\n",
    "  * Special_char_count\n",
    "  * Numeric_char_count\n",
    "  * Alpha_char_count\n",
    "  * Entropy (Shannon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by Step:\n",
    "\n",
    "# 1. Import log file\n",
    "# 2. Extract features from log file to be used for classification\n",
    "# 3. Load trained classifier(s)\n",
    "# 4. Classify log file\n",
    "# 5. Keep attack-related logs\n",
    "\n",
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>query[A] 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>forwarded 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>reply 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>query[A] 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>forwarded 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp                                            message\n",
       "0  Jan 21 00:00:09  query[A] 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1z...\n",
       "1  Jan 21 00:00:09  forwarded 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1...\n",
       "2  Jan 21 00:00:09  reply 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*...\n",
       "3  Jan 21 00:00:31  query[A] 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pW...\n",
       "4  Jan 21 00:00:31  forwarded 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/p..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import log file\n",
    "\n",
    "path_inet_dnsmasq = \"/gather/inet-firewall/logs/dnsmasq.log\"\n",
    "df_inet_dnsmasq = load_data_robust(PATH_DATASET_TO_CLASSIFY + path_inet_dnsmasq)\n",
    "\n",
    "df_inet_dnsmasq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 1 rows (0.00%) didn't match any pattern\n",
      "Unmatched rows:\n",
      "  Row 17181: failed to access /etc/dnsmasq.d/dnsmasq-resolv.conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 2. Extract features from log file to be used for classification\n",
    "\n",
    "from helpers.inet_dnsmasq_log_helper import extract_dns_features\n",
    "\n",
    "df_inet_dnsmasq_features = extract_dns_features(df_inet_dnsmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset Count:  275900\n",
      "Dataset after feature extraction:  275899\n",
      "WARNING: Dataset length mismatch\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole dataset Count: \", df_inet_dnsmasq.__len__())\n",
    "print(\"Dataset after feature extraction: \", df_inet_dnsmasq_features.__len__())\n",
    "\n",
    "if(df_inet_dnsmasq.__len__() != df_inet_dnsmasq_features.__len__()):\n",
    "    print(\"WARNING: Dataset length mismatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_length</th>\n",
       "      <th>domain_parts</th>\n",
       "      <th>avg_part_length</th>\n",
       "      <th>max_part_length</th>\n",
       "      <th>special_char_count</th>\n",
       "      <th>numeric_char_count</th>\n",
       "      <th>alpha_char_count</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query_a</td>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>17.545455</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>5.768733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forwarded</td>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>17.545455</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>5.768733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reply</td>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>17.545455</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>5.768733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>query_a</td>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pWGilEyrR-....</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>17.545455</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>134</td>\n",
       "      <td>5.763903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forwarded</td>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pWGilEyrR-....</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>17.545455</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>134</td>\n",
       "      <td>5.763903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275894</th>\n",
       "      <td>query_a</td>\n",
       "      <td>Jan 24 23:55:43</td>\n",
       "      <td>e6410.d.akamaiedge.net</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3.572624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275895</th>\n",
       "      <td>forwarded</td>\n",
       "      <td>Jan 24 23:55:43</td>\n",
       "      <td>e6410.d.akamaiedge.net</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3.572624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275896</th>\n",
       "      <td>reply</td>\n",
       "      <td>Jan 24 23:55:43</td>\n",
       "      <td>e6410.d.akamaiedge.net</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>3.572624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275897</th>\n",
       "      <td>query_aaaa</td>\n",
       "      <td>Jan 24 23:58:27</td>\n",
       "      <td>mail</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275898</th>\n",
       "      <td>cached</td>\n",
       "      <td>Jan 24 23:58:27</td>\n",
       "      <td>mail</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275899 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_type        timestamp  \\\n",
       "0           query_a  Jan 21 00:00:09   \n",
       "1         forwarded  Jan 21 00:00:09   \n",
       "2             reply  Jan 21 00:00:09   \n",
       "3           query_a  Jan 21 00:00:31   \n",
       "4         forwarded  Jan 21 00:00:31   \n",
       "...             ...              ...   \n",
       "275894      query_a  Jan 24 23:55:43   \n",
       "275895    forwarded  Jan 24 23:55:43   \n",
       "275896        reply  Jan 24 23:55:43   \n",
       "275897   query_aaaa  Jan 24 23:58:27   \n",
       "275898       cached  Jan 24 23:58:27   \n",
       "\n",
       "                                                   domain  domain_length  \\\n",
       "0       3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....            203   \n",
       "1       3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....            203   \n",
       "2       3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*Wgpq-....            203   \n",
       "3       3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pWGilEyrR-....            203   \n",
       "4       3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pWGilEyrR-....            203   \n",
       "...                                                   ...            ...   \n",
       "275894                             e6410.d.akamaiedge.net             22   \n",
       "275895                             e6410.d.akamaiedge.net             22   \n",
       "275896                             e6410.d.akamaiedge.net             22   \n",
       "275897                                               mail              4   \n",
       "275898                                               mail              4   \n",
       "\n",
       "        domain_parts  avg_part_length  max_part_length  special_char_count  \\\n",
       "0                 11        17.545455               35                  13   \n",
       "1                 11        17.545455               35                  13   \n",
       "2                 11        17.545455               35                  13   \n",
       "3                 11        17.545455               35                  17   \n",
       "4                 11        17.545455               35                  17   \n",
       "...              ...              ...              ...                 ...   \n",
       "275894             4         4.750000               10                   0   \n",
       "275895             4         4.750000               10                   0   \n",
       "275896             4         4.750000               10                   0   \n",
       "275897             1         4.000000                4                   0   \n",
       "275898             1         4.000000                4                   0   \n",
       "\n",
       "        numeric_char_count  alpha_char_count   entropy  \n",
       "0                       30               150  5.768733  \n",
       "1                       30               150  5.768733  \n",
       "2                       30               150  5.768733  \n",
       "3                       42               134  5.763903  \n",
       "4                       42               134  5.763903  \n",
       "...                    ...               ...       ...  \n",
       "275894                   4                15  3.572624  \n",
       "275895                   4                15  3.572624  \n",
       "275896                   4                15  3.572624  \n",
       "275897                   0                 4  2.000000  \n",
       "275898                   0                 4  2.000000  \n",
       "\n",
       "[275899 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inet_dnsmasq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded santos_dnsmasq_preprocessor1 from trained-models/inet_dnsmasq_log/more\\santos_dnsmasq_preprocessor1.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.05036893,  1.89019734,  1.93916484, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.05036893,  1.89019734,  1.93916484, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.05036893,  1.89019734,  1.93916484, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [-0.51002789, -0.39718352, -0.54399952, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.76465298, -1.3774896 , -0.68954912, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.76465298, -1.3774896 , -0.68954912, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data before classification\n",
    "\n",
    "preprocessor = load_models_from_disk(\"trained-models/inet_dnsmasq_log/more\")[\"santos_dnsmasq_preprocessor1\"]\n",
    "\n",
    "df_inet_dnsmasq_features = df_inet_dnsmasq_features.drop([\"timestamp\"], axis=1)\n",
    "df_inet_dnsmasq_features = preprocessor.fit_transform(df_inet_dnsmasq_features)\n",
    "\n",
    "df_inet_dnsmasq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded santos_dnsmasq_rf1 from trained-models/inet_dnsmasq_log\\santos_dnsmasq_rf1.joblib\n"
     ]
    }
   ],
   "source": [
    "# 3. Load trained classifier(s) and preprocessor\n",
    "# Only one: Random Forest Multiclass Classifier\n",
    "\n",
    "models = load_models_from_disk(\"trained-models/inet_dnsmasq_log\")\n",
    "clf_rf = models[\"santos_dnsmasq_rf1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Classify log file\n",
    "y_pred = clf_rf.predict(df_inet_dnsmasq_features)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label classification (one-hot encoded labels)\n",
    "label_columns = [\"attacker\", \"dns_scan\", \"dnsteal\", \"dnsteal-received\", \n",
    "                 \"escalate\", \"foothold\", \"network_scan\", \n",
    "                 \"service_scan\", \"traceroute\", \"webshell_cmd\", \"wpscan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Predicted label counts:\n",
      "               label  count\n",
      "0           attacker  52985\n",
      "2            dnsteal  52985\n",
      "3   dnsteal-received  52985\n",
      "1           dns_scan   1755\n",
      "5           foothold   1526\n",
      "4           escalate      0\n",
      "6       network_scan      0\n",
      "7       service_scan      0\n",
      "8         traceroute      0\n",
      "9       webshell_cmd      0\n",
      "10            wpscan      0\n",
      "\n",
      "2. Distribution of number of labels per instance (0 means benign):\n",
      "   num_labels   count  percentage\n",
      "0           0  221145       80.15\n",
      "1           1     257        0.09\n",
      "2           2    1512        0.55\n",
      "3           3   52985       19.20\n",
      "\n",
      "3. Co-occurrence matrix:\n",
      "                  attacker  dns_scan  dnsteal  dnsteal-received  escalate  \\\n",
      "attacker           52985.0       0.0  52985.0           52985.0       0.0   \n",
      "dns_scan               0.0    1755.0      0.0               0.0       0.0   \n",
      "dnsteal            52985.0       0.0  52985.0           52985.0       0.0   \n",
      "dnsteal-received   52985.0       0.0  52985.0           52985.0       0.0   \n",
      "escalate               0.0       0.0      0.0               0.0       0.0   \n",
      "foothold               0.0    1512.0      0.0               0.0       0.0   \n",
      "network_scan           0.0       0.0      0.0               0.0       0.0   \n",
      "service_scan           0.0       0.0      0.0               0.0       0.0   \n",
      "traceroute             0.0       0.0      0.0               0.0       0.0   \n",
      "webshell_cmd           0.0       0.0      0.0               0.0       0.0   \n",
      "wpscan                 0.0       0.0      0.0               0.0       0.0   \n",
      "\n",
      "                  foothold  network_scan  service_scan  traceroute  \\\n",
      "attacker               0.0           0.0           0.0         0.0   \n",
      "dns_scan            1512.0           0.0           0.0         0.0   \n",
      "dnsteal                0.0           0.0           0.0         0.0   \n",
      "dnsteal-received       0.0           0.0           0.0         0.0   \n",
      "escalate               0.0           0.0           0.0         0.0   \n",
      "foothold            1526.0           0.0           0.0         0.0   \n",
      "network_scan           0.0           0.0           0.0         0.0   \n",
      "service_scan           0.0           0.0           0.0         0.0   \n",
      "traceroute             0.0           0.0           0.0         0.0   \n",
      "webshell_cmd           0.0           0.0           0.0         0.0   \n",
      "wpscan                 0.0           0.0           0.0         0.0   \n",
      "\n",
      "                  webshell_cmd  wpscan  \n",
      "attacker                   0.0     0.0  \n",
      "dns_scan                   0.0     0.0  \n",
      "dnsteal                    0.0     0.0  \n",
      "dnsteal-received           0.0     0.0  \n",
      "escalate                   0.0     0.0  \n",
      "foothold                   0.0     0.0  \n",
      "network_scan               0.0     0.0  \n",
      "service_scan               0.0     0.0  \n",
      "traceroute                 0.0     0.0  \n",
      "webshell_cmd               0.0     0.0  \n",
      "wpscan                     0.0     0.0  \n"
     ]
    }
   ],
   "source": [
    "# Some evaluation:\n",
    "\n",
    "# 1. Basic counts for each class\n",
    "predicted_label_counts = pd.DataFrame({\n",
    "    'label': label_columns,\n",
    "    'count': np.sum(y_pred, axis=0)\n",
    "})\n",
    "predicted_label_counts = predicted_label_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"1. Predicted label counts:\")\n",
    "print(predicted_label_counts)\n",
    "\n",
    "\n",
    "# 2. Distribution of number of labels per instance\n",
    "labels_per_instance = np.sum(y_pred, axis=1)\n",
    "label_distribution = pd.Series(labels_per_instance).value_counts().sort_index()\n",
    "label_distribution_df = pd.DataFrame({\n",
    "    'num_labels': label_distribution.index,\n",
    "    'count': label_distribution.values,\n",
    "    'percentage': (label_distribution.values / len(y_pred) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"\\n2. Distribution of number of labels per instance (0 means benign):\")\n",
    "print(label_distribution_df)\n",
    "\n",
    "\n",
    "# 3. Co-occurrence matrix - which labels appear together\n",
    "cooccurrence = np.zeros((len(label_columns), len(label_columns)))\n",
    "for i in range(len(label_columns)):\n",
    "    for j in range(len(label_columns)):\n",
    "        if i == j:\n",
    "            # Count instances where this label appears\n",
    "            cooccurrence[i, j] = np.sum(y_pred[:, i])\n",
    "        else:\n",
    "            # Count instances where both labels appear together\n",
    "            cooccurrence[i, j] = np.sum(np.logical_and(y_pred[:, i], y_pred[:, j]))\n",
    "\n",
    "cooccurrence_df = pd.DataFrame(cooccurrence, \n",
    "                               index=label_columns,\n",
    "                               columns=label_columns)\n",
    "\n",
    "print(\"\\n3. Co-occurrence matrix:\")\n",
    "print(cooccurrence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>message</th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>attacker</th>\n",
       "      <th>dns_scan</th>\n",
       "      <th>dnsteal</th>\n",
       "      <th>dnsteal-received</th>\n",
       "      <th>escalate</th>\n",
       "      <th>foothold</th>\n",
       "      <th>network_scan</th>\n",
       "      <th>service_scan</th>\n",
       "      <th>traceroute</th>\n",
       "      <th>webshell_cmd</th>\n",
       "      <th>wpscan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>query[A] 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1z...</td>\n",
       "      <td>[attacker, dnsteal, dnsteal-received]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>forwarded 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1...</td>\n",
       "      <td>[attacker, dnsteal, dnsteal-received]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 21 00:00:09</td>\n",
       "      <td>reply 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*...</td>\n",
       "      <td>[attacker, dnsteal, dnsteal-received]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>query[A] 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pW...</td>\n",
       "      <td>[attacker, dnsteal, dnsteal-received]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 21 00:00:31</td>\n",
       "      <td>forwarded 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/p...</td>\n",
       "      <td>[attacker, dnsteal, dnsteal-received]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp                                            message  \\\n",
       "0  Jan 21 00:00:09  query[A] 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1z...   \n",
       "1  Jan 21 00:00:09  forwarded 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1...   \n",
       "2  Jan 21 00:00:09  reply 3x6-.596-.IunWTzebVlyAhhHj*ZfWjOBun1zAf*...   \n",
       "3  Jan 21 00:00:31  query[A] 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/pW...   \n",
       "4  Jan 21 00:00:31  forwarded 3x6-.597-.L**fA/ib4pGEIb5*uJ223L5A/p...   \n",
       "\n",
       "                        predicted_labels  attacker  dns_scan  dnsteal  \\\n",
       "0  [attacker, dnsteal, dnsteal-received]         1         0        1   \n",
       "1  [attacker, dnsteal, dnsteal-received]         1         0        1   \n",
       "2  [attacker, dnsteal, dnsteal-received]         1         0        1   \n",
       "3  [attacker, dnsteal, dnsteal-received]         1         0        1   \n",
       "4  [attacker, dnsteal, dnsteal-received]         1         0        1   \n",
       "\n",
       "   dnsteal-received  escalate  foothold  network_scan  service_scan  \\\n",
       "0                 1         0         0             0             0   \n",
       "1                 1         0         0             0             0   \n",
       "2                 1         0         0             0             0   \n",
       "3                 1         0         0             0             0   \n",
       "4                 1         0         0             0             0   \n",
       "\n",
       "   traceroute  webshell_cmd  wpscan  \n",
       "0           0             0       0  \n",
       "1           0             0       0  \n",
       "2           0             0       0  \n",
       "3           0             0       0  \n",
       "4           0             0       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Keep attack-related logs\n",
    "\n",
    "# Get the indices where at least one class is predicted\n",
    "positive_indices = np.where(np.sum(y_pred, axis=1) > 0)[0]\n",
    "\n",
    "# Create a new dataframe with just those entries\n",
    "df_attack_related_intranet_auth = df_inet_dnsmasq.iloc[positive_indices].copy()\n",
    "\n",
    "# Add a column with the predicted classes for each entry\n",
    "df_attack_related_intranet_auth['predicted_labels'] = [\n",
    "    [label_columns[j] for j in range(len(label_columns)) if y_pred[i, j] == 1]\n",
    "    for i in positive_indices\n",
    "]\n",
    "\n",
    "# Add individual class columns if needed\n",
    "for i, label in enumerate(label_columns):\n",
    "    df_attack_related_intranet_auth[label] = y_pred[positive_indices, i]\n",
    "\n",
    "# Preview the result\n",
    "df_attack_related_intranet_auth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54754\n",
      "275900\n",
      "275899\n"
     ]
    }
   ],
   "source": [
    "print(df_attack_related_intranet_auth.__len__())\n",
    "print(df_inet_dnsmasq.__len__())\n",
    "print(df_inet_dnsmasq_features.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get events from file: intranet / access.log\n",
    "* Contributes:\n",
    "\n",
    "* Trained Classifiers: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by Step:\n",
    "\n",
    "# 1. Import log file\n",
    "# 2. Extract features from log file to be used for classification\n",
    "# 3. Load trained classifier(s)\n",
    "# 4. Classify log file\n",
    "# 5. Keep attack-related logs\n",
    "\n",
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../AIT_LD-v2/russellmitchell/gather/intranet_server/logs/apache2/intranet.smith.santos.com-access.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Import log file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path_intranet_access_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gather/intranet_server/logs/apache2/intranet.smith.santos.com-access.log\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df_intranet_access \u001b[38;5;241m=\u001b[39m load_data_robust(PATH_DATASET_TO_CLASSIFY \u001b[38;5;241m+\u001b[39m path_intranet_access_log)\n\u001b[0;32m      6\u001b[0m df_intranet_access\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32md:\\Timba\\Studium\\Masterarbeit\\Repo\\IDSAlertCorrelaction_CySec_MasterThesis\\helpers\\data_helper.py:25\u001b[0m, in \u001b[0;36mload_data_robust\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mLoads and processes log files in both comma-separated and space-separated formats.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    pandas.DataFrame: Processed DataFrame with 'timestamp' and 'message' columns\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# First, read the first line of the file to determine format\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     26\u001b[0m     first_line \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Check if the format uses commas\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../AIT_LD-v2/russellmitchell/gather/intranet_server/logs/apache2/intranet.smith.santos.com-access.log'"
     ]
    }
   ],
   "source": [
    "# 1. Import log file\n",
    "\n",
    "path_intranet_access_log = \"/gather/intranet_server/logs/apache2/intranet.smith.santos.com-access.log\"\n",
    "df_intranet_access = load_data_robust(PATH_DATASET_TO_CLASSIFY + path_intranet_access_log)\n",
    "\n",
    "df_intranet_access.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract features from log file to be used for classification\n",
    "\n",
    "from helpers.intranet_access_log_helper import extract_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load trained classifier(s)\n",
    "# 4. Classify log file\n",
    "# 5. Keep attack-related logs\n",
    "\n",
    "#6. TODO: Think about how to correlate them\n",
    "\n",
    "#7. Upload Iris Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
